{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6ZJE1ifYLUS"
      },
      "outputs": [],
      "source": [
        "!pip install watermark -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext watermark\n",
        "%watermark\n",
        "%watermark --iversions"
      ],
      "metadata": {
        "id": "E-UP2VduZIjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1df8753-8a8f-4903-fff9-00f1218a0a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last updated: 2025-08-26T23:24:19.965877+00:00\n",
            "\n",
            "Python implementation: CPython\n",
            "Python version       : 3.12.11\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "Compiler    : GCC 11.4.0\n",
            "OS          : Linux\n",
            "Release     : 6.1.123+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy -q"
      ],
      "metadata": {
        "id": "sH75BW_vZmJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_lg -q"
      ],
      "metadata": {
        "id": "OponjIHpZp7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cfe0bb1-bccc-46d7-e95b-b77c6fb67460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "e7I_GB_EZxO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"es_core_news_lg\")\n",
        "import es_core_news_lg\n",
        "nlp = es_core_news_lg.load()"
      ],
      "metadata": {
        "id": "FMptUiDxonAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caso 1"
      ],
      "metadata": {
        "id": "WnbRl9YvqATr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'Yo, Matías Barreto, estoy volando hacia Buenos Aires.')\n",
        "print([w.text for w in doc])"
      ],
      "metadata": {
        "id": "E2Sj7MXlZ08E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3b3fb8d-42f4-4f9a-b8df-5b635aed2a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Yo', ',', 'Matías', 'Barreto', ',', 'estoy', 'volando', 'hacia', 'Buenos', 'Aires', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of sentence\n",
        "print(\"La cantidad de tokens es: \", len(doc))"
      ],
      "metadata": {
        "id": "qIgo7RQvaG0L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb18d885-c564-4b42-bf82-a833d45f97bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La cantidad de tokens es:  11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print individual words (i.e., tokens)\n",
        "print(\"Los tokens son: \\n\" + \"_\" * 20 + \"\\n\")\n",
        "for words in doc:\n",
        "    print(words)"
      ],
      "metadata": {
        "id": "GE4EjJ13pUqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e993aab1-4410-4f67-a50c-8d94e6499ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Los tokens son: \n",
            "____________________\n",
            "\n",
            "Yo\n",
            ",\n",
            "Matías\n",
            "Barreto\n",
            ",\n",
            "estoy\n",
            "volando\n",
            "hacia\n",
            "Buenos\n",
            "Aires\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token.text, token.lemma_)"
      ],
      "metadata": {
        "id": "QtVw-81CpwIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ecded0d-3e58-4af3-c7f5-ed90acf8461e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yo yo\n",
            ", ,\n",
            "Matías Matías\n",
            "Barreto Barreto\n",
            ", ,\n",
            "estoy estar\n",
            "volando volar\n",
            "hacia hacia\n",
            "Buenos Buenos\n",
            "Aires Aires\n",
            ". .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterar sobre los tokens en el documento y mostrar el texto y su etiqueta POS\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, spacy.explain(token.pos_))"
      ],
      "metadata": {
        "id": "V4Ej8SLNnJ73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f16ea46c-3e81-49a4-c509-d9129fe38e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yo PRON pronoun\n",
            ", PUNCT punctuation\n",
            "Matías PROPN proper noun\n",
            "Barreto PROPN proper noun\n",
            ", PUNCT punctuation\n",
            "estoy AUX auxiliary\n",
            "volando VERB verb\n",
            "hacia ADP adposition\n",
            "Buenos PROPN proper noun\n",
            "Aires PROPN proper noun\n",
            ". PUNCT punctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "lkNXIwoNqSjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq = Counter(doc)\n",
        "print(word_freq)"
      ],
      "metadata": {
        "id": "ueuaUKa4qUyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa89c0eb-1e50-4345-d6e3-33d870abf8ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({Yo: 1, ,: 1, Matías: 1, Barreto: 1, ,: 1, estoy: 1, volando: 1, hacia: 1, Buenos: 1, Aires: 1, .: 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caso 2"
      ],
      "metadata": {
        "id": "W5GDn44hqEgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'En abril de 2025 estaremos realizando una introducción a spaCy para trabajar conceptos basicos de PNL')\n",
        "for token in doc:\n",
        "  print(token.text, token.lemma_)"
      ],
      "metadata": {
        "id": "vhrutm8jZ456",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce062b27-2275-460d-8dec-bb789688663d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En en\n",
            "abril abril\n",
            "de de\n",
            "2025 2025\n",
            "estaremos estarer\n",
            "realizando realizar\n",
            "una uno\n",
            "introducción introducción\n",
            "a a\n",
            "spaCy spacy\n",
            "para para\n",
            "trabajar trabajar\n",
            "conceptos concepto\n",
            "basicos basico\n",
            "de de\n",
            "PNL PNL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sentences[5])\n",
        "print(doc.text)\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "id": "uUE5NFzqyK9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "aea09fa6-9125-4e7e-f5b9-5b646657e512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sentences' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3306973126.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdep_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.es.examples import sentences\n",
        "\n",
        "doc = nlp(sentences[5])\n",
        "print(doc.text)\n",
        "for token in doc:\n",
        "    print(token.text, spacy.explain(token.pos_), token.dep_)"
      ],
      "metadata": {
        "id": "bbM0D0bmaMPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "P3CqaZRzaPXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq = Counter(doc)\n",
        "print(word_freq)"
      ],
      "metadata": {
        "id": "xoVm0gpxaR0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 120})"
      ],
      "metadata": {
        "id": "L19H3yIKysQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caso 3"
      ],
      "metadata": {
        "id": "dC5GPSd6qq9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy"
      ],
      "metadata": {
        "id": "MkH-S-r-aUOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Queremos viajar desde Buenos Aires a Mar del Plata y unos dias depues a La Plata\"\n",
        "doc = nlp(text)\n",
        "displacy.render(doc,style='ent',jupyter=True,options={'distance':200})"
      ],
      "metadata": {
        "id": "Pd7BK_FfaWp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Su nombre es Benjamin; nació en Argentina.\"\n",
        "doc = nlp(text)\n",
        "displacy.render(doc,style='ent',jupyter=True,options={'distance':200})"
      ],
      "metadata": {
        "id": "5Tcs0F7VtiHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 120})"
      ],
      "metadata": {
        "id": "Uh8RTazpzAXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caso 4"
      ],
      "metadata": {
        "id": "DIatGR6l0A9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in nlp(\"Mis padres y amigos viven en Buenos Aires.\"):\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "UInoA0jm0Cro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in nlp(\"Mis padres y amigos viven en Buenos Aires.\").noun_chunks:\n",
        "      print(chunk.text)"
      ],
      "metadata": {
        "id": "afG9vo9x0R-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caso 4"
      ],
      "metadata": {
        "id": "7gFuHVVqqv0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.symbols import ORTH, NORM"
      ],
      "metadata": {
        "id": "1SGQbJUDaaMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u\"Estoy volando hacia Baires\")\n",
        "print([w.text for w in doc])"
      ],
      "metadata": {
        "id": "He6gbguIabQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.language import Language\n",
        "from spacy.tokens import Token"
      ],
      "metadata": {
        "id": "uESY-lJ0adli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Componente personalizado para asignar lemas específicos\n",
        "@Language.component(\"custom_lemmatizer\")\n",
        "def custom_lemmatizer(doc):\n",
        "    for token in doc:\n",
        "        if token.text == \"Baires\":\n",
        "            token.lemma_ = \"Buenos Aires\"\n",
        "    return doc\n",
        "\n",
        "# Remove the existing custom component if it exists\n",
        "if \"custom_lemmatizer\" in nlp.pipe_names:\n",
        "    nlp.remove_pipe(\"custom_lemmatizer\")\n",
        "\n",
        "# Añadir el componente al pipeline\n",
        "nlp.add_pipe(\"custom_lemmatizer\", after='lemmatizer')\n",
        "\n",
        "# Ejemplo de texto en español\n",
        "doc = nlp(u'Estoy volando a Baires')\n",
        "print([w.text for w in doc])\n",
        "print([w.lemma_ for w in doc])"
      ],
      "metadata": {
        "id": "d-GrUsA6aiO2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}